{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reads in a domain and problem file, and generates a networkx graph from the entire search space by performing a breadth first search until a solution is found."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyperplan import _parse, _ground\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prime number generator later used by hashing function to uniquely represent states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_primes():\n",
    "    \"\"\" Generate an infinite sequence of prime numbers.\n",
    "    \"\"\"\n",
    "    # Maps composites to primes witnessing their compositeness.\n",
    "    # This is memory efficient, as the sieve is not \"run forward\"\n",
    "    # indefinitely, but only as long as required by the current\n",
    "    # number being tested.\n",
    "    #\n",
    "    D = {}\n",
    "    \n",
    "    # The running integer that's checked for primeness\n",
    "    q = 2\n",
    "    \n",
    "    while True:\n",
    "        if q not in D:\n",
    "            # q is a new prime.\n",
    "            # Yield it and mark its first multiple that isn't\n",
    "            # already marked in previous iterations\n",
    "            # \n",
    "            yield q\n",
    "            D[q * q] = [q]\n",
    "        else:\n",
    "            # q is composite. D[q] is the list of primes that\n",
    "            # divide it. Since we've reached q, we no longer\n",
    "            # need it in the map, but we'll mark the next \n",
    "            # multiples of its witnesses to prepare for larger\n",
    "            # numbers\n",
    "            # \n",
    "            for p in D[q]:\n",
    "                D.setdefault(p + q, []).append(p)\n",
    "            del D[q]\n",
    "        \n",
    "        q += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the PDDL problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "pddl_dir = \"pddl_files\"\n",
    "domain_path = \"logistics/43/domain.pddl\"\n",
    "problem_path = \"logistics/43/problogistics-6-1.pddl\"\n",
    "problem = _parse(domain_file=os.path.join(pddl_dir, domain_path),problem_file=os.path.join(pddl_dir, problem_path))\n",
    "task = _ground(problem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'(at apn1 apt1)',\n",
       " '(at apn1 apt2)',\n",
       " '(at apn1 pos1)',\n",
       " '(at apn1 pos2)',\n",
       " '(at obj11 apt1)',\n",
       " '(at obj11 apt2)',\n",
       " '(at obj11 pos1)',\n",
       " '(at obj11 pos2)',\n",
       " '(at obj12 apt1)',\n",
       " '(at obj12 apt2)',\n",
       " '(at obj12 pos1)',\n",
       " '(at obj12 pos2)',\n",
       " '(at obj13 apt1)',\n",
       " '(at obj13 apt2)',\n",
       " '(at obj13 pos1)',\n",
       " '(at obj13 pos2)',\n",
       " '(at obj21 apt1)',\n",
       " '(at obj21 apt2)',\n",
       " '(at obj21 pos1)',\n",
       " '(at obj21 pos2)',\n",
       " '(at obj22 apt1)',\n",
       " '(at obj22 apt2)',\n",
       " '(at obj22 pos1)',\n",
       " '(at obj22 pos2)',\n",
       " '(at obj23 apt1)',\n",
       " '(at obj23 apt2)',\n",
       " '(at obj23 pos1)',\n",
       " '(at obj23 pos2)',\n",
       " '(at tru1 apt1)',\n",
       " '(at tru1 apt2)',\n",
       " '(at tru1 pos1)',\n",
       " '(at tru1 pos2)',\n",
       " '(at tru2 apt1)',\n",
       " '(at tru2 apt2)',\n",
       " '(at tru2 pos1)',\n",
       " '(at tru2 pos2)',\n",
       " '(in obj11 apn1)',\n",
       " '(in obj11 tru1)',\n",
       " '(in obj11 tru2)',\n",
       " '(in obj12 apn1)',\n",
       " '(in obj12 tru1)',\n",
       " '(in obj12 tru2)',\n",
       " '(in obj13 apn1)',\n",
       " '(in obj13 tru1)',\n",
       " '(in obj13 tru2)',\n",
       " '(in obj21 apn1)',\n",
       " '(in obj21 tru1)',\n",
       " '(in obj21 tru2)',\n",
       " '(in obj22 apn1)',\n",
       " '(in obj22 tru1)',\n",
       " '(in obj22 tru2)',\n",
       " '(in obj23 apn1)',\n",
       " '(in obj23 tru1)',\n",
       " '(in obj23 tru2)'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task.facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['apn1',\n",
       " 'apt2',\n",
       " 'pos2',\n",
       " 'apt1',\n",
       " 'pos1',\n",
       " 'cit2',\n",
       " 'cit1',\n",
       " 'tru2',\n",
       " 'tru1',\n",
       " 'obj23',\n",
       " 'obj22',\n",
       " 'obj21',\n",
       " 'obj13',\n",
       " 'obj12',\n",
       " 'obj11']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "objects = list(problem.objects.keys())\n",
    "objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['package',\n",
       " 'truck',\n",
       " 'airplane',\n",
       " 'airport',\n",
       " 'location',\n",
       " 'in-city',\n",
       " 'city',\n",
       " 'at',\n",
       " 'in']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicates = list(problem.domain.predicates.keys())\n",
    "predicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['load-truck',\n",
       " 'load-airplane',\n",
       " 'unload-truck',\n",
       " 'unload-airplane',\n",
       " 'drive-truck',\n",
       " 'fly-airplane']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actions = list(problem.domain.actions.keys())\n",
    "actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'load-truck': 2,\n",
       " 'load-airplane': 3,\n",
       " 'unload-truck': 5,\n",
       " 'unload-airplane': 7,\n",
       " 'drive-truck': 11,\n",
       " 'fly-airplane': 13,\n",
       " 'apn1': 17,\n",
       " 'apt2': 19,\n",
       " 'pos2': 23,\n",
       " 'apt1': 29,\n",
       " 'pos1': 31,\n",
       " 'cit2': 37,\n",
       " 'cit1': 41,\n",
       " 'tru2': 43,\n",
       " 'tru1': 47,\n",
       " 'obj23': 53,\n",
       " 'obj22': 59,\n",
       " 'obj21': 61,\n",
       " 'obj13': 67,\n",
       " 'obj12': 71,\n",
       " 'obj11': 73,\n",
       " 'package': 79,\n",
       " 'truck': 83,\n",
       " 'airplane': 89,\n",
       " 'airport': 97,\n",
       " 'location': 101,\n",
       " 'in-city': 103,\n",
       " 'city': 107,\n",
       " 'at': 109,\n",
       " 'in': 113}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens = actions + objects + predicates\n",
    "token_mapping = {token: p for token, p in zip(tokens, gen_primes())}\n",
    "token_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hash_state(state):\n",
    "    hash = 1\n",
    "    \n",
    "    if type(state) == tuple:\n",
    "        temp = state[0].name[1:-1].split(\" \")[0]\n",
    "        hash *= token_mapping[temp]\n",
    "        \n",
    "        for fact in state[1]:\n",
    "            temp_tokens = fact[1:-1]\n",
    "            temp_tokens = temp_tokens.split(\" \")\n",
    "\n",
    "            for temp_token in temp_tokens:\n",
    "                hash *= token_mapping[temp_token]        \n",
    "        \n",
    "    else:    \n",
    "        for fact in state:\n",
    "            temp_tokens = fact[1:-1]\n",
    "            temp_tokens = temp_tokens.split(\" \")\n",
    "\n",
    "            for temp_token in temp_tokens:\n",
    "                hash *= token_mapping[temp_token]\n",
    "            \n",
    "    return hash"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By modifying the BFS function that's already a part of pyperplan, we can just hook in some code that builds up a networkx graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import deque\n",
    "import logging\n",
    "\n",
    "from search import searchspace\n",
    "\n",
    "\n",
    "def breadth_first_search(planning_task):\n",
    "    '''\n",
    "    Searches for a plan on the given task using breadth first search and\n",
    "    duplicate detection.\n",
    "\n",
    "    @param planning_task: The planning task to solve.\n",
    "    @return: The solution as a list of operators or None if the task is\n",
    "    unsolvable.\n",
    "    '''\n",
    "    # counts the number of loops (only for printing)\n",
    "    G = nx.Graph()\n",
    "    iteration = 0\n",
    "    # fifo-queue storing the nodes which are next to explore\n",
    "    queue = deque()\n",
    "    queue.append(searchspace.make_root_node(planning_task.initial_state))\n",
    "    # set storing the explored nodes, used for duplicate detection\n",
    "    closed = {planning_task.initial_state}\n",
    "    while queue:\n",
    "        iteration += 1\n",
    "        logging.debug(\"breadth_first_search: Iteration %d, #unexplored=%d\"\n",
    "                      % (iteration, len(queue)))\n",
    "        # get the next node to explore\n",
    "        node = queue.popleft()\n",
    "        # exploring the node or if it is a goal node extracting the plan\n",
    "        if planning_task.goal_reached(node.state):\n",
    "            logging.info(\"Goal reached. Start extraction of solution.\")\n",
    "            logging.info(\"%d Nodes expanded\" % iteration)\n",
    "            return G\n",
    "        for operator, successor_state in planning_task.get_successor_states(\n",
    "                                                                   node.state):\n",
    "            # duplicate detection\n",
    "            if successor_state not in closed:\n",
    "                new_node = searchspace.make_child_node(node, operator,\n",
    "                                                         successor_state)\n",
    "                queue.append(new_node)\n",
    "                G.add_edge(hash_state(node.state), hash_state(new_node.state))\n",
    "                 # remember the successor state\n",
    "                # print(\"node.g: {} | new_node.g: {}\".format(node.g, new_node.g))\n",
    "                closed.add(successor_state)\n",
    "    logging.info(\"No operators left. Task unsolvable.\")\n",
    "    logging.info(\"%d Nodes expanded\" % iteration)\n",
    "    return G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = breadth_first_search(task)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since prime number state representation results in ridiculously large numbers, we map these numbers to smaller ones by creating a new graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_G = nx.Graph()\n",
    "node_mapping = {n: i for i, n in enumerate(list(G.nodes))}\n",
    "\n",
    "for edge in list(G.edges):\n",
    "    new_G.add_edge(node_mapping[edge[0]], node_mapping[edge[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1991"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_G.nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save output in format required by c++ implementation of node2vec from snap-stanford"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_dir =  \"graphs\"\n",
    "graph_file = os.path.join(graph_dir, os.path.dirname(problem_path))\n",
    "\n",
    "if not os.path.exists(graph_file):\n",
    "    os.makedirs(graph_file)\n",
    "    \n",
    "graph_file = os.path.join(graph_file, os.path.basename(problem_path).split(\".\")[0] + \".edgelist\")\n",
    "\n",
    "with open(graph_file, \"w\") as f:\n",
    "    for edge in list(new_G.edges):\n",
    "        f.write(\"{} {}\\n\".format(edge[0], edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn",
   "language": "python",
   "name": "gnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
