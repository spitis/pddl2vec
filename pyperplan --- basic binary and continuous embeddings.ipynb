{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyperplan import _parse, _ground\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "problem = _parse(domain_file='testdomain.pddl',problem_file='testinstance.pddl')\n",
    "task = _ground(problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive embedding\n",
    "\n",
    "- Naive binary embedding for a state space with N fluents is a N-dimensional binary vector that has a 1 for each true fact\n",
    "- Naive continuous embedding is just the binary embedding times an NxM trainable embedding matrix\n",
    "- Naive because we aren't using any shared feature representations for similar predicates/facts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_naive_embedding_fn(facts):\n",
    "  \"\"\"NOTE: Probably not best implementation atm; should be faster to initialize np.zeros, then \n",
    "  get indices of the state with a hash/dict, and set those indices to 1\"\"\"\n",
    "  facts = list(facts)\n",
    "  def naive_embedding(state):\n",
    "    return np.array([(fact in state) for fact in facts], dtype=np.float32)\n",
    "  return naive_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "naive_emb = create_naive_embedding_fn(task.facts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 1., 0., 0., 0., 0., 0., 1., 1., 1., 0., 0., 1.],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# So the binary embedding of the initial state is:\n",
    "naive_emb(task.initial_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.01330587,  0.00131232, -0.02270757,  0.01784888,  0.05183817,\n",
       "        -0.01747334, -0.04812877, -0.03406031, -0.02066482,  0.00268202,\n",
       "        -0.01011077,  0.0104214 ,  0.05642818, -0.01336079,  0.04047353,\n",
       "        -0.0518132 ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can now get an N-dimensional continuous embedding using a trainable embedding matrix\n",
    "# This is equivalent to having an N-dimensional embedding for each \"fact/fluent\", and then adding them together.\n",
    "N = 16\n",
    "W_emb = np.random.normal(0., 1e-2, (len(task.facts), N))\n",
    "\n",
    "# So now the initial embedding of the initial state is:\n",
    "np.expand_dims(naive_emb(task.initial_state), 0).dot(W_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smarter embedding\n",
    "\n",
    "- this is non-trivial... we should discuss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
